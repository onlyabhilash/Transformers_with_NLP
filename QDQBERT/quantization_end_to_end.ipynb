{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edx0YNpNnT6Q"
      },
      "source": [
        "# Method to perform Nvidia GPU INT-8 quantization on most transformers model (encoder based)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyeRu2VCnT6Y"
      },
      "source": [
        "Quantization is one of the most effective and generic approach to make model inference faster.\n",
        "Basically, it replaces high precision float numbers in model tensors encoded in 32 or 16 bits by lower precision ones encoded in 8 bits or less:\n",
        "\n",
        "* it takes less memory\n",
        "* computation is easier / faster\n",
        "\n",
        "It can be applied to any model in theory, and, if done well, it should not decrease its accuracy.\n",
        "\n",
        "The purpose of this notebook is to show 2 processes to perform quantization on most `transformer` architectures.\n",
        "\n",
        "**TL;DR, we benchmarked Pytorch and Nvidia TensorRT, on both CPU and GPU, with/without quantization, our methods provide the fastest inference by large margin**.\n",
        "\n",
        "| Framework                  | Precision | Latency (ms) | Accuracy | Speedup    | Hardware |\n",
        "|:---------------------------|-----------|--------------|----------|:-----------|:--------:|\n",
        "| Pytorch                    | FP32      | 4000         | 86.8 %   | X 0.02     |   CPU    |\n",
        "| Pytorch                    | FP16      | 4005         | 86.8 %   | X 0.02     |   CPU    |\n",
        "| Pytorch                    | **INT-8** | 3670         | 86.8 %   | X 0.02     | **CPU**  |\n",
        "| Pytorch                    | FP32      | 80           | 86.8 %   | X 1        |   GPU    |\n",
        "| Pytorch                    | FP16      | 58           | 86.8 %   | X 1.38     |   GPU    |\n",
        "| ONNX Runtime               | FP32      | 74           | 86.8 %   | X 1.08     |   GPU    |\n",
        "| ONNX Runtime               | FP16      | 34           | 86.8 %   | X 2.35     |   GPU    |\n",
        "| ONNX Runtime               | FP32      | 3767         | 86.8 %   | X 0.02     |   CPU    |\n",
        "| ONNX Runtime               | FP16      | 4607         | 86.8 %   | X 0.02     |   CPU    |\n",
        "| ONNX Runtime               | **INT-8** | 3712         | 86.8 %   | X 0.02     | **CPU**  |\n",
        "| TensorRT                   | FP16      | 30           | 86.8 %   | X 2.67     |   GPU    |\n",
        "| TensorRT (**our method 1**)| **INT-8** | 15           | 84.4 %   | **X 5.33** | **GPU**  |\n",
        "| TensorRT (**our method 2**)| **INT-8** | 16           | 85.8 %   | **X 5.00** | **GPU**  |\n",
        "\n",
        "> measures done on a Nvidia RTX 3090 GPU + 12 cores i7 Intel CPU (support AVX-2 instructions)\n",
        ">\n",
        "> architecture `Roberta-base` with batch of size 32 / seq len 256, similar results obtained for other sizes/seq len not included in the table.\n",
        ">\n",
        "> accuracy obtained after a single epoch, no LR search or any hyper parameter optimization\n",
        ">\n",
        "> CPU measures are a bit unfair, it's still possible to push performance a bit by adding lots of (Python related) complexities and using last generation CPU, still those measurements are indicative of orders of magnitude to expect from Pytorch+CPU deployment.\n",
        ">\n",
        "> same kind of acceleration is observed on all seq len / batch sizes\n",
        "\n",
        "\n",
        "## A (very) short intro to INT-8 quantization\n",
        "\n",
        "Basic idea behind model quantization is to replace tensors made of float numbers (usually encoded on 32 bits) by lower precision representation (integers encoded on 8 bits for Nvidia GPUs).\n",
        "Therefore computation is faster and model memory footprint is lower. Making tensor storage smaller makes memory transfer faster... and is also a source of computation acceleration.\n",
        "This technic is very interesting for its trade-off: you reduce inference time significantly, and when dataset is large enough, it costs close to nothing in accuracy.\n",
        "\n",
        "Replacing float numbers by integers is done through a mapping.\n",
        "This step is called `calibration`, and its purpose is to compute for each tensor or each channel of a tensor (one of its dimensions) a range of all possible values and then define a scale and a distribution center to map float numbers to 8 bits integers.\n",
        "The process is well described in this [Nvidia presentation](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf).\n",
        "\n",
        "There are several ways to perform quantization, depending of how and when the `calibration` is performed:\n",
        "\n",
        "* dynamically: the mapping is done during the inference, there are some overhead but it's easy to put in place and usually the accuracy is preserved,\n",
        "* statically, after training (`post training quantization` or `PTQ`): this way is efficient, but it may have a significant accuracy cost,\n",
        "* statically, before training (`quantization aware training` or `QAT`): this way is efficient and has a low accuracy cost as the weights will take care of the result\n",
        "\n",
        "In this guide we will focus on the third option: `QAT`.\n",
        "\n",
        "During the quantization aware *training*:\n",
        "\n",
        "* in the inside, Pytorch will train with high precision float numbers,\n",
        "* on the outside, Pytorch will simulate that a quantization has already been applied and output results accordingly (for loss computation for instance)\n",
        "\n",
        "The simulation process is done through the add of quantization / dequantization nodes, most often called `QDQ`, it's an abbreviation you will see often in quantization world.\n",
        "\n",
        "You can check this [high quality blog post](https://leimao.github.io/article/Neural-Networks-Quantization/) for more information.\n",
        "\n",
        "## Why this notebook?\n",
        "\n",
        "CPU quantization is supported out of the box by `Pytorch` and `ONNX Runtime`.\n",
        "**GPU quantization on the other side requires specific tools and process to be applied**.\n",
        "\n",
        "In the specific case of `transformer` models, until recently (december 2021), the only way shown by Nvidia is to build manually the graph of our models in `TensorRT`. This is a low level approach, based on GPU capacity knowledge (which operators are supported, etc.). It's certainly out of reach of most NLP practitioners and is very time consuming to update/adapt to new architectures.\n",
        "\n",
        "Hopefully, Nvidia added to Hugging Face `transformer` library a new model called `QDQBert` few weeks ago.\n",
        "Basically, it's a vanilla `Bert` architecture which supports INT-8 quantization.\n",
        "It doesn't support any other architecture out of the box, like `Albert`, `Roberta`, or `Electra`.\n",
        "Nvidia also provide a demo dedicated to the SQuaD task.\n",
        "\n",
        "This open the door to extension of the approach to other architectures.\n",
        "\n",
        "To be both simple and cover most use cases, in this notebook we will see:\n",
        "\n",
        "* how to perform GPU quantization on **any** transformer model (not just Bert) using a simple trick, a `transplatation`\n",
        "* how to perform GPU quantization on `QDQRoberta`, a custom model similar to `QDQBert` and supported by `transformer-deploy` library\n",
        "* how to apply quantization to a common task like classification (which is easier to understand than question answering)\n",
        "* measure performance gain (latency)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8_ayKxSnT6e"
      },
      "source": [
        "## Project setup\n",
        "\n",
        "### Dependencies installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqrgI7a1nT6f"
      },
      "source": [
        "We install `master` branch of `transfomers` library to use a new model: **QDQBert** and `transformer-deploy` to leverage `TensorRT` models (TensorRT API is not something simple to master, it's highly advised to use a wrapper). Your machine should have Nvidia CUDA 11.X, TensorRT 8.2.1 and cuBLAS installed. It's said to be tricky to install, in my experience, just follow Nvidia instructions **and nothing else**, it should work out of the box. Docker image with TensorRT 8.2.1 has not yet been released, this notebook will be updated when it's ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "#! pip install git+https://github.com/huggingface/transformers\n",
        "#! pip install git+https://github.com/ELS-RD/transformer-deploy\n",
        "#! pip install sklearn datasets\n",
        "#! pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com\n",
        "# or install pytorch-quantization from https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN81_KgTnT6i"
      },
      "source": [
        "Check the GPU is enabled and usable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OzrD4f-3ydk",
        "outputId": "54cc2ea6-6969-4e01-f9f9-78c5fc91ff85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Dec  9 19:47:47 2021       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:03:00.0  On |                  N/A |\r\n",
            "| 67%   55C    P8    45W / 350W |    286MiB / 24267MiB |      6%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|    0   N/A  N/A      1944      G   /usr/lib/xorg/Xorg                148MiB |\r\n",
            "|    0   N/A  N/A      7816      G   /usr/bin/gnome-shell               40MiB |\r\n",
            "|    0   N/A  N/A    529613      G   ...518105.log --shared-files       13MiB |\r\n",
            "|    0   N/A  N/A    540908      G   ...AAAAAAAAA= --shared-files       49MiB |\r\n",
            "|    0   N/A  N/A   1378576      G   ...AAAAAAAAA= --shared-files       31MiB |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPMoLPBn_1vN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import transformers\n",
        "import datasets\n",
        "from typing import OrderedDict as OD, List, Dict, Union\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    PreTrainedModel,\n",
        "    QDQBertForSequenceClassification,\n",
        "    BertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    IntervalStrategy,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformer_deploy.QDQModels.QDQRoberta import QDQRobertaForSequenceClassification\n",
        "import pytorch_quantization.nn as quant_nn\n",
        "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
        "from pytorch_quantization import calib\n",
        "import logging\n",
        "from datasets import DatasetDict\n",
        "from transformer_deploy.backends.trt_utils import build_engine, get_binding_idxs, infer_tensorrt, load_engine\n",
        "from transformer_deploy.backends.ort_utils import convert_to_onnx\n",
        "from collections import OrderedDict\n",
        "from transformer_deploy.benchmarks.utils import track_infer_time, print_timings\n",
        "from pycuda._driver import Stream\n",
        "import tensorrt as trt\n",
        "from tensorrt.tensorrt import IExecutionContext, Logger, Runtime\n",
        "import pycuda.autoinit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHNSNjrqnT6o"
      },
      "source": [
        "Set logging to `error` to make the `notebook` more readable on Github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1O30yAknT6p"
      },
      "outputs": [],
      "source": [
        "log_level = logging.ERROR\n",
        "logging.getLogger().setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
        "transformers.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adl-XobknT6r"
      },
      "source": [
        "This part is inspired from an [official Notebooks from Hugging Face](https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "task = \"mnli\"\n",
        "num_labels = 3\n",
        "model_checkpoint = \"roberta-base\"\n",
        "batch_size = 32\n",
        "max_seq_len = 256\n",
        "validation_key = \"validation_matched\"\n",
        "timings: Dict[str, List[float]] = dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "264fc00f-1b35-474b-c699-ccf07387ed70",
        "colab": {
          "referenced_widgets": [
            "ff977d13b16d44ddbf9536d565248621"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff977d13b16d44ddbf9536d565248621",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 392702\n",
              "    })\n",
              "    validation_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9815\n",
              "    })\n",
              "    validation_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9832\n",
              "    })\n",
              "    test_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9796\n",
              "    })\n",
              "    test_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9847\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"glue\", task)\n",
        "metric = load_metric(\"glue\", task)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True` and `padding=\"max_length\"`. This will ensure that all sequences have the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_seq_len\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LWCIStxpnT6w"
      },
      "source": [
        "Some functions required for training and exporting the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_eROEu0znT6x"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if task != \"stsb\":\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "    else:\n",
        "        predictions = predictions[:, 0]\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "def calibrate(model: PreTrainedModel, encoded_dataset: DatasetDict, nb_sample: int = 128) -> PreTrainedModel:\n",
        "    # Find the TensorQuantizer and enable calibration\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, quant_nn.TensorQuantizer):\n",
        "            if module._calibrator is not None:\n",
        "                module.disable_quant()\n",
        "                module.enable_calib()\n",
        "            else:\n",
        "                module.disable()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start_index in tqdm(range(0, nb_sample, batch_size)):\n",
        "            end_index = start_index + batch_size\n",
        "            data = encoded_dataset[\"train\"][start_index:end_index]\n",
        "            input_torch = {\n",
        "                k: torch.tensor(v, dtype=torch.long, device=\"cpu\")\n",
        "                for k, v in data.items()\n",
        "                if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
        "            }\n",
        "            model(**input_torch)\n",
        "\n",
        "    # Finalize calibration\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, quant_nn.TensorQuantizer):\n",
        "            if module._calibrator is not None:\n",
        "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
        "                    module.load_calib_amax()\n",
        "                else:\n",
        "                    module.load_calib_amax(\"percentile\", percentile=99.99)\n",
        "                module.enable_quant()\n",
        "                module.disable_calib()\n",
        "            else:\n",
        "                module.enable()\n",
        "\n",
        "    model.cuda()\n",
        "    return model\n",
        "\n",
        "\n",
        "def convert_tensor(data: OD[str, List[List[int]]], output: str) -> OD[str, Union[np.ndarray, torch.Tensor]]:\n",
        "    input: OD[str, Union[np.ndarray, torch.Tensor]] = OrderedDict()\n",
        "    for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
        "        if k in data:\n",
        "            v = data[k]\n",
        "            if output == \"torch\":\n",
        "                value = torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
        "            elif output == \"np\":\n",
        "                value = np.asarray(v, dtype=np.int32)\n",
        "            else:\n",
        "                raise Exception(f\"unknown output type: {output}\")\n",
        "            input[k] = value\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_J6JqrZnT6y"
      },
      "source": [
        "Some `TensorRT` reused variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAS_GrtVnT6y"
      },
      "outputs": [],
      "source": [
        "runtime: Runtime = trt.Runtime(trt_logger)\n",
        "profile_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGuApTSEnT61"
      },
      "source": [
        "Measure accuracy for ONNX Runtime and TensorRT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouDmLFAUnT62"
      },
      "outputs": [],
      "source": [
        "validation_labels = [item[\"label\"] for item in encoded_dataset[validation_key]]\n",
        "\n",
        "\n",
        "def measure_accuracy(infer, int64: bool) -> float:\n",
        "    outputs = list()\n",
        "    for start_index in tqdm(range(0, len(encoded_dataset[validation_key]), batch_size)):\n",
        "        end_index = start_index + batch_size\n",
        "        data = encoded_dataset[validation_key][start_index:end_index]\n",
        "        inputs: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\")\n",
        "        if int64:\n",
        "            for k, v in inputs.items():\n",
        "                inputs[k] = v.astype(np.int64)\n",
        "        output = infer(inputs)\n",
        "        output = np.argmax(output[0], axis=1).astype(int).tolist()\n",
        "        outputs.extend(output)\n",
        "    return np.mean(np.array(outputs) == np.array(validation_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JinTvg6nT62"
      },
      "source": [
        "## Fine-tuning model\n",
        "\n",
        "Now that our data are ready, we can download the pretrained model and fine-tune it.\n",
        "\n",
        "Default parameters to be used for the training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYsjp7YBnT63"
      },
      "outputs": [],
      "source": [
        "nb_step = 1000\n",
        "strategy = IntervalStrategy.STEPS\n",
        "args = TrainingArguments(\n",
        "    f\"{model_checkpoint}-{task}\",\n",
        "    evaluation_strategy=strategy,\n",
        "    eval_steps=nb_step,\n",
        "    logging_steps=nb_step,\n",
        "    save_steps=nb_step,\n",
        "    save_strategy=strategy,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size * 2,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    group_by_length=True,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=[],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cpd9GqnT64"
      },
      "source": [
        "## Method 1: `Transplantation` of weights from a source model to an optimized architecture\n",
        "\n",
        "Transplantation idea is to export weights from one model and use them in another one.\n",
        "In our case, the source are `Roberta` weights and the target is `Bert` archtecture which is highly optimized on `TensorRT` for GPU quantization.\n",
        "\n",
        "Indeed, not all models are quantization compliant. The optimization engine (`TensorRT`) search for some patterns and will fail to opimize the model if it doesn't find them. It requires the Pytorch code to be written in a certain way and use certain operations. For that reason, it's a good idea to reuse an architecture highly optimized.\n",
        "\n",
        "We will leverage the fact that since `Bert` have been released, very few improvements have been brought to the transformer architecture (at least for encoder only models).\n",
        "Better models appeared, and most of the work has been done to improve the pretraining step (aka the weights).\n",
        "So the idea will be to take the weights from those new models and put them inside `Bert` architecture.\n",
        "\n",
        "The process described below should work for most architectures.\n",
        "\n",
        "**steps**:\n",
        "\n",
        "* load `Bert` model\n",
        "* retrieve layer/weight names\n",
        "* load target model (here `Roberta`)\n",
        "* replace weight/layer names with those from `Roberta`\n",
        "* override the architecture name in model configuration\n",
        "\n",
        "If there is no 1 to 1 correspondance (it happens), try to keep at least token embeddings and self attention. Of course, it's possible that if a model is very different, the transplant may cost some accuracy. In our experience, if your trainset is big enough it should not happen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4z_YNuvsnT64"
      },
      "outputs": [],
      "source": [
        "model_bert: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=num_labels\n",
        ")\n",
        "bert_keys = list(model_bert.state_dict().keys())\n",
        "del model_bert\n",
        "\n",
        "model_roberta: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=num_labels\n",
        ")\n",
        "model_roberta.save_pretrained(\"roberta-in-bert\")\n",
        "del model_roberta\n",
        "model_weights: OD[str, Tensor] = torch.load(\"roberta-in-bert/pytorch_model.bin\")\n",
        "\n",
        "# Roberta -> Bert, there is 1 to 1 correspondance, for other models, you may need to create your own mapping.\n",
        "for bert_key in bert_keys:\n",
        "    # pop remove the first weights from the Ordered dict ...\n",
        "    _, weight = model_weights.popitem(last=False)\n",
        "    # ... and we re-insert them, in order, with a new key\n",
        "    model_weights[bert_key] = weight\n",
        "\n",
        "# we re-export the weights\n",
        "torch.save(model_weights, \"roberta-in-bert/pytorch_model.bin\")\n",
        "del model_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104jG17DnT64"
      },
      "source": [
        "We override the architecture name to make `transformers` believe it is `Bert`..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2yIH5HXnT65"
      },
      "outputs": [],
      "source": [
        "# =====> change architecture to bert base <======\n",
        "import json\n",
        "\n",
        "with open(\"roberta-in-bert/config.json\") as f:\n",
        "    content = json.load(f)\n",
        "    content[\"architectures\"] = [\"bert\"]\n",
        "\n",
        "with open(\"roberta-in-bert/config.json\", mode=\"w\") as f:\n",
        "    json.dump(content, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-jLKoV7nT65"
      },
      "source": [
        "## Model training\n",
        "\n",
        "The goal of this first training is to update weights to the new architecture to help the next step, the calibration.\n",
        "Indeed, `Roberta` architecture is a bit different from vanilla `Bert`, for instance position embeddings are not managed the same way, as they are at the very bottom of the model, they impact all model layers.\n",
        "If we skip this step, the value ranges computed during the calibration step may be very wrong and the `QAT` would provide low accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mHKyCrqnT65",
        "outputId": "69556a39-b0c8-439d-babf-c02c6f86a15c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|trainer.py:437] 2021-12-09 19:48:11,412 >> Using amp half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7474, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08}\n",
            "{'eval_loss': 0.5083153247833252, 'eval_accuracy': 0.8007131940906775, 'eval_runtime': 18.9412, 'eval_samples_per_second': 518.182, 'eval_steps_per_second': 8.13, 'epoch': 0.08}\n",
            "{'loss': 0.5457, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16}\n",
            "{'eval_loss': 0.47982481122016907, 'eval_accuracy': 0.8163015792154865, 'eval_runtime': 19.417, 'eval_samples_per_second': 505.486, 'eval_steps_per_second': 7.931, 'epoch': 0.16}\n",
            "{'loss': 0.5075, 'learning_rate': 7.557855280312908e-06, 'epoch': 0.24}\n",
            "{'eval_loss': 0.4634084105491638, 'eval_accuracy': 0.8218033622007132, 'eval_runtime': 19.6824, 'eval_samples_per_second': 498.67, 'eval_steps_per_second': 7.824, 'epoch': 0.24}\n",
            "{'loss': 0.483, 'learning_rate': 6.743807040417211e-06, 'epoch': 0.33}\n",
            "{'eval_loss': 0.42370399832725525, 'eval_accuracy': 0.8344370860927153, 'eval_runtime': 19.5405, 'eval_samples_per_second': 502.29, 'eval_steps_per_second': 7.881, 'epoch': 0.33}\n",
            "{'loss': 0.4652, 'learning_rate': 5.9289439374185145e-06, 'epoch': 0.41}\n",
            "{'eval_loss': 0.4242672026157379, 'eval_accuracy': 0.8365766683647479, 'eval_runtime': 18.8626, 'eval_samples_per_second': 520.343, 'eval_steps_per_second': 8.164, 'epoch': 0.41}\n",
            "{'loss': 0.451, 'learning_rate': 5.114080834419818e-06, 'epoch': 0.49}\n",
            "{'eval_loss': 0.42570847272872925, 'eval_accuracy': 0.8365766683647479, 'eval_runtime': 18.9979, 'eval_samples_per_second': 516.636, 'eval_steps_per_second': 8.106, 'epoch': 0.49}\n",
            "{'loss': 0.4505, 'learning_rate': 4.299217731421121e-06, 'epoch': 0.57}\n",
            "{'eval_loss': 0.4005618989467621, 'eval_accuracy': 0.8451349974528782, 'eval_runtime': 18.8952, 'eval_samples_per_second': 519.443, 'eval_steps_per_second': 8.15, 'epoch': 0.57}\n",
            "{'loss': 0.4422, 'learning_rate': 3.4859843546284226e-06, 'epoch': 0.65}\n",
            "{'eval_loss': 0.3936935365200043, 'eval_accuracy': 0.8445236882322975, 'eval_runtime': 18.9752, 'eval_samples_per_second': 517.253, 'eval_steps_per_second': 8.116, 'epoch': 0.65}\n",
            "{'loss': 0.4332, 'learning_rate': 2.6711212516297265e-06, 'epoch': 0.73}\n",
            "{'eval_loss': 0.3954601585865021, 'eval_accuracy': 0.8470708099847173, 'eval_runtime': 18.7912, 'eval_samples_per_second': 522.32, 'eval_steps_per_second': 8.195, 'epoch': 0.73}\n",
            "{'loss': 0.4254, 'learning_rate': 1.8570730117340288e-06, 'epoch': 0.81}\n",
            "{'eval_loss': 0.39208605885505676, 'eval_accuracy': 0.8492103922567499, 'eval_runtime': 19.1256, 'eval_samples_per_second': 513.185, 'eval_steps_per_second': 8.052, 'epoch': 0.81}\n",
            "{'loss': 0.4263, 'learning_rate': 1.0422099087353325e-06, 'epoch': 0.9}\n",
            "{'eval_loss': 0.384700208902359, 'eval_accuracy': 0.851044319918492, 'eval_runtime': 19.1997, 'eval_samples_per_second': 511.207, 'eval_steps_per_second': 8.021, 'epoch': 0.9}\n",
            "{'loss': 0.4263, 'learning_rate': 2.2816166883963498e-07, 'epoch': 0.98}\n",
            "{'eval_loss': 0.38375720381736755, 'eval_accuracy': 0.8508405501782985, 'eval_runtime': 18.5961, 'eval_samples_per_second': 527.8, 'eval_steps_per_second': 8.281, 'epoch': 0.98}\n",
            "{'train_runtime': 2701.3089, 'train_samples_per_second': 145.375, 'train_steps_per_second': 4.543, 'train_loss': 0.48233796906751014, 'epoch': 1.0}\n",
            "{'eval_loss': 0.384700208902359, 'eval_accuracy': 0.851044319918492, 'eval_runtime': 18.5921, 'eval_samples_per_second': 527.911, 'eval_steps_per_second': 8.283, 'epoch': 1.0}\n",
            "{'eval_loss': 0.384700208902359, 'eval_accuracy': 0.851044319918492, 'eval_runtime': 18.5921, 'eval_samples_per_second': 527.911, 'eval_steps_per_second': 8.283, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "transformers.logging.set_verbosity_error()\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\"roberta-in-bert\", num_labels=num_labels)\n",
        "model_bert = model_bert.cuda()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_bert,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[validation_key],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "transformers.logging.set_verbosity_error()\n",
        "trainer.train()\n",
        "print(trainer.evaluate())\n",
        "model_bert.save_pretrained(\"roberta-in-bert-trained\")\n",
        "del trainer\n",
        "del model_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FFtrLD-nT65"
      },
      "source": [
        "## Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQD5yT-nT66"
      },
      "source": [
        "Below we will start the quantization process.\n",
        "It follow those steps:\n",
        "\n",
        "* perform the calibration\n",
        "* perform a quantization aware training\n",
        "\n",
        "By passing validation values to the model, we will calibrate it, meaning it will get the right range / scale to convert FP32 weights to int-8 ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrSUl6yPnT66"
      },
      "source": [
        "### Calibration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lthmLxGrnT66"
      },
      "source": [
        "#### Activate histogram calibration\n",
        "\n",
        "There are several kinds of calbrators, below we use the percentile one (99.99p) (`histogram`), basically, its purpose is to just remove the most extreme values before computing range / scale.\n",
        "The other option in NLP is `max`, it's much faster but expect lower accuracy.\n",
        "\n",
        "Second calibration option, choose between calibration done at the tensor level or per channel (finer grained value ranges, a bit slower).\n",
        "Calibration is based on few samples (in our case 128 sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBW7qf06nT66"
      },
      "outputs": [],
      "source": [
        "# you can also use \"max\" instead of \"historgram\"\n",
        "input_desc = QuantDescriptor(num_bits=8, calib_method=\"histogram\")\n",
        "# below we do per-channel quantization for weights, set axis to None to get a per tensor calibration\n",
        "weight_desc = QuantDescriptor(num_bits=8, axis=(0,))\n",
        "quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)\n",
        "quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Gy6LA4nT68"
      },
      "source": [
        "#### Perform calibration\n",
        "\n",
        "During this step we will enable the calibration nodes, and pass some representative data to the model.\n",
        "It will then be used to compute the scale/range.\n",
        "\n",
        "Official recommendations from Nvidia is to calibrate over thousands of examples from the validation set.\n",
        "Here we use 128 examples because it's a slow process. It's enough to be close from the original accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZAoYs2cQnT68",
        "outputId": "62454f78-b72f-4b39-9169-b429937abb05",
        "colab": {
          "referenced_widgets": [
            "776c4daba3a04b34a72210697ad37e6a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "776c4daba3a04b34a72210697ad37e6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# keep it on CPU\n",
        "model_q = QDQBertForSequenceClassification.from_pretrained(\"roberta-in-bert-trained\", num_labels=num_labels)\n",
        "model_q = calibrate(model=model_q, encoded_dataset=encoded_dataset)\n",
        "model_q.save_pretrained(\"roberta-in-bert-trained-quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKIMjpX3nT69"
      },
      "source": [
        "### Quantization Aware Training (QAT)\n",
        "\n",
        "The query aware training is not a mandatory step, but **highly** recommended to get the best accuracy. Basically we will redo the training with the quantization enabled and a low learning rate to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf",
        "outputId": "48d4f8ab-392c-432e-a446-0cdd250b7bd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|trainer.py:437] 2021-12-09 20:39:35,717 >> Using amp half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4307818114757538, 'eval_accuracy': 0.8327050433010698, 'eval_runtime': 45.8394, 'eval_samples_per_second': 214.117, 'eval_steps_per_second': 3.36}\n",
            "{'eval_loss': 0.4307818114757538, 'eval_accuracy': 0.8327050433010698, 'eval_runtime': 45.8394, 'eval_samples_per_second': 214.117, 'eval_steps_per_second': 3.36}\n",
            "{'loss': 0.4542, 'learning_rate': 9.187581486310299e-07, 'epoch': 0.08}\n",
            "{'eval_loss': 0.4258573651313782, 'eval_accuracy': 0.8376974019358125, 'eval_runtime': 46.5114, 'eval_samples_per_second': 211.023, 'eval_steps_per_second': 3.311, 'epoch': 0.08}\n",
            "{'loss': 0.4422, 'learning_rate': 8.372718383311604e-07, 'epoch': 0.16}\n",
            "{'eval_loss': 0.4214017987251282, 'eval_accuracy': 0.8395313295975547, 'eval_runtime': 46.5334, 'eval_samples_per_second': 210.924, 'eval_steps_per_second': 3.309, 'epoch': 0.16}\n",
            "{'loss': 0.4268, 'learning_rate': 7.557855280312907e-07, 'epoch': 0.24}\n",
            "{'eval_loss': 0.41808152198791504, 'eval_accuracy': 0.8423841059602649, 'eval_runtime': 46.5578, 'eval_samples_per_second': 210.813, 'eval_steps_per_second': 3.308, 'epoch': 0.24}\n",
            "{'loss': 0.4223, 'learning_rate': 6.742992177314211e-07, 'epoch': 0.33}\n",
            "{'eval_loss': 0.42257484793663025, 'eval_accuracy': 0.838920020376974, 'eval_runtime': 46.599, 'eval_samples_per_second': 210.627, 'eval_steps_per_second': 3.305, 'epoch': 0.33}\n",
            "{'loss': 0.4282, 'learning_rate': 5.928943937418513e-07, 'epoch': 0.41}\n",
            "{'eval_loss': 0.41137370467185974, 'eval_accuracy': 0.8409577177789098, 'eval_runtime': 46.5435, 'eval_samples_per_second': 210.878, 'eval_steps_per_second': 3.309, 'epoch': 0.41}\n",
            "{'loss': 0.4234, 'learning_rate': 5.114895697522817e-07, 'epoch': 0.49}\n",
            "{'eval_loss': 0.41403627395629883, 'eval_accuracy': 0.841874681609781, 'eval_runtime': 46.5743, 'eval_samples_per_second': 210.739, 'eval_steps_per_second': 3.307, 'epoch': 0.49}\n",
            "{'loss': 0.4202, 'learning_rate': 4.3000325945241197e-07, 'epoch': 0.57}\n",
            "{'eval_loss': 0.4175918698310852, 'eval_accuracy': 0.8417727967396842, 'eval_runtime': 46.5654, 'eval_samples_per_second': 210.779, 'eval_steps_per_second': 3.307, 'epoch': 0.57}\n",
            "{'loss': 0.4289, 'learning_rate': 3.4859843546284223e-07, 'epoch': 0.65}\n",
            "{'eval_loss': 0.41122177243232727, 'eval_accuracy': 0.8441161487519103, 'eval_runtime': 46.5376, 'eval_samples_per_second': 210.905, 'eval_steps_per_second': 3.309, 'epoch': 0.65}\n",
            "{'loss': 0.4283, 'learning_rate': 2.6711212516297263e-07, 'epoch': 0.73}\n",
            "{'eval_loss': 0.4128565490245819, 'eval_accuracy': 0.8426897605705552, 'eval_runtime': 46.7511, 'eval_samples_per_second': 209.942, 'eval_steps_per_second': 3.294, 'epoch': 0.73}\n",
            "{'loss': 0.4174, 'learning_rate': 1.8570730117340285e-07, 'epoch': 0.81}\n",
            "{'eval_loss': 0.40628135204315186, 'eval_accuracy': 0.8441161487519103, 'eval_runtime': 46.6972, 'eval_samples_per_second': 210.184, 'eval_steps_per_second': 3.298, 'epoch': 0.81}\n",
            "{'loss': 0.4191, 'learning_rate': 1.0422099087353324e-07, 'epoch': 0.9}\n",
            "{'eval_loss': 0.41109439730644226, 'eval_accuracy': 0.8451349974528782, 'eval_runtime': 46.5565, 'eval_samples_per_second': 210.819, 'eval_steps_per_second': 3.308, 'epoch': 0.9}\n",
            "{'loss': 0.4178, 'learning_rate': 2.2734680573663624e-08, 'epoch': 0.98}\n",
            "{'eval_loss': 0.409859836101532, 'eval_accuracy': 0.8464595007641366, 'eval_runtime': 46.5572, 'eval_samples_per_second': 210.816, 'eval_steps_per_second': 3.308, 'epoch': 0.98}\n",
            "{'train_runtime': 4943.333, 'train_samples_per_second': 79.441, 'train_steps_per_second': 2.483, 'train_loss': 0.4271986904169155, 'epoch': 1.0}\n",
            "{'eval_loss': 0.409859836101532, 'eval_accuracy': 0.8464595007641366, 'eval_runtime': 46.5126, 'eval_samples_per_second': 211.018, 'eval_steps_per_second': 3.311, 'epoch': 1.0}\n",
            "{'eval_loss': 0.409859836101532, 'eval_accuracy': 0.8464595007641366, 'eval_runtime': 46.5126, 'eval_samples_per_second': 211.018, 'eval_steps_per_second': 3.311, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "model_q = QDQBertForSequenceClassification.from_pretrained(\"roberta-in-bert-trained-quantized\", num_labels=num_labels)\n",
        "model_q = model_q.cuda()\n",
        "\n",
        "args.learning_rate = 1e-6\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_q,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[validation_key],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "transformers.logging.set_verbosity_error()\n",
        "print(trainer.evaluate())\n",
        "trainer.train()\n",
        "print(trainer.evaluate())\n",
        "model_q.save_pretrained(\"roberta-in-bert-trained-quantized-bis\")\n",
        "del model_q\n",
        "del trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdsODEz4nT69"
      },
      "source": [
        "### Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_nPgvWJnT69"
      },
      "source": [
        "#### Export a `QDQ Pytorch` model on `ONNX`, we need to enable fake quantization mode from Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X32GOZl9nT6-",
        "outputId": "743a8dc1-f08f-43f3-a3fc-1ebb25f283c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  inputs, amax.item() / bound, 0,\n",
            "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n"
          ]
        }
      ],
      "source": [
        "data = encoded_dataset[\"train\"][0:3]\n",
        "input_torch = convert_tensor(data, output=\"torch\")\n",
        "\n",
        "model_q = QDQBertForSequenceClassification.from_pretrained(\n",
        "    \"roberta-in-bert-trained-quantized-bis\", num_labels=num_labels\n",
        ")\n",
        "model_q = model_q.cuda()\n",
        "from pytorch_quantization.nn import TensorQuantizer\n",
        "\n",
        "TensorQuantizer.use_fb_fake_quant = True\n",
        "convert_to_onnx(model_q, output_path=\"model_q.onnx\", inputs_pytorch=input_torch, opset=13)\n",
        "TensorQuantizer.use_fb_fake_quant = False\n",
        "# del model_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "WJU3Y-GwnT6-"
      },
      "source": [
        "#### Convert `ONNX` graph to `TensorRT` engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UUtMVMLynT6-"
      },
      "outputs": [],
      "source": [
        "engine = build_engine(\n",
        "    runtime=runtime,\n",
        "    onnx_file_path=\"model_q.onnx\",\n",
        "    logger=trt_logger,\n",
        "    min_shape=(1, max_seq_len),  # 1 in batch size to support batch from size 1 to 32\n",
        "    optimal_shape=(batch_size, max_seq_len),\n",
        "    max_shape=(batch_size, max_seq_len),\n",
        "    workspace_size=10000 * 1024 * 1024,\n",
        "    fp16=False,\n",
        "    int8=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "id": "FmzC77P8nT6-"
      },
      "outputs": [],
      "source": [
        "# same thing from command line\n",
        "# !/usr/src/tensorrt/bin/trtexec --onnx=model_q.onnx --shapes=input_ids:32x256,attention_mask:32x256 --int8 --workspace=10000 --saveEngine=\"test.plan\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4RPo-DCnnT6_"
      },
      "source": [
        "#### Prepare input and output buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QOd2-f4ynT6_"
      },
      "outputs": [],
      "source": [
        "stream: Stream = pycuda.driver.Stream()\n",
        "context: IExecutionContext = engine.create_execution_context()\n",
        "context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle)\n",
        "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index)  # type: List[int], List[int]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BOP4Z6iMnT6_"
      },
      "outputs": [],
      "source": [
        "data = encoded_dataset[\"train\"][0:batch_size]\n",
        "input_np: Dict[str, np.ndarray] = convert_tensor(data, output=\"np\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hSsZDp0OnT6_"
      },
      "source": [
        "#### Inference on `TensorRT`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm_fATGpnT6_"
      },
      "source": [
        "We first check that inference is working correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AEBB-hmEnT7A",
        "outputId": "2892fb71-5178-4f00-d100-3a490308f952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[ 0.74428475,  0.92069125, -1.8263749 ],\n",
            "       [ 1.851499  , -1.0653014 , -1.4172065 ],\n",
            "       [ 1.7828548 , -1.1306885 , -1.1852558 ],\n",
            "       [ 1.6740881 , -0.7420906 , -1.5647126 ],\n",
            "       [ 2.2817519 ,  0.01615529, -2.7685544 ],\n",
            "       [ 3.1013348 , -0.48788828, -3.3121827 ],\n",
            "       [-3.0679533 ,  2.708288  ,  0.70968   ],\n",
            "       [ 3.1545656 , -1.0913979 , -2.6073706 ],\n",
            "       [-0.3026344 , -1.7703965 ,  1.6011946 ],\n",
            "       [-3.2131557 , -0.5275665 ,  3.786335  ],\n",
            "       [ 2.2266033 , -1.2310914 , -1.523544  ],\n",
            "       [-1.5110059 , -0.46988845,  1.7940781 ],\n",
            "       [-2.4409676 ,  3.7142613 , -0.73455316],\n",
            "       [-1.8158143 ,  1.9259161 , -0.05558195],\n",
            "       [-0.33427513, -0.48280472,  0.6140785 ],\n",
            "       [ 2.3686104 , -1.4665173 , -1.5184819 ],\n",
            "       [ 3.58267   , -1.1251179 , -3.060151  ],\n",
            "       [-2.4983776 , -2.0526152 ,  4.5359097 ],\n",
            "       [-3.441052  , -0.6358736 ,  4.1798487 ],\n",
            "       [-2.2326443 ,  4.032728  , -1.1005057 ],\n",
            "       [ 3.4742196 , -0.98982847, -3.2408576 ],\n",
            "       [ 1.7075734 ,  0.56745094, -2.7780871 ],\n",
            "       [-2.6132822 ,  0.45791242,  2.1319566 ],\n",
            "       [ 3.498353  , -0.68513054, -3.4510155 ],\n",
            "       [ 3.394199  , -1.578492  , -2.5097256 ],\n",
            "       [-1.5231444 ,  0.22112232,  1.1882032 ],\n",
            "       [-2.7878394 ,  1.368547  ,  1.5938892 ],\n",
            "       [-2.263415  ,  2.5507202 , -0.16721557],\n",
            "       [-2.716222  ,  0.03395515,  2.6644425 ],\n",
            "       [ 2.663493  , -0.7295195 , -2.7137947 ],\n",
            "       [ 2.6217816 , -0.7861772 , -2.417176  ],\n",
            "       [ 2.506748  , -0.09974011, -3.0284772 ]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "tensorrt_output = infer_tensorrt(\n",
        "    context=context,\n",
        "    host_inputs=input_np,\n",
        "    input_binding_idxs=input_binding_idxs,\n",
        "    output_binding_idxs=output_binding_idxs,\n",
        "    stream=stream,\n",
        ")\n",
        "print(tensorrt_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTIuRmIjnT7A"
      },
      "source": [
        "Measure of the accuracy when `TensorRT` is the engine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwW8L_U9nT7A",
        "outputId": "24812f5a-d263-428a-8faa-d6ff67e5e7ee",
        "colab": {
          "referenced_widgets": [
            "e3123eb9ee20476aa55e70b181c124d7"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3123eb9ee20476aa55e70b181c124d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/307 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.8444218033622007"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "infer_trt = lambda inputs: infer_tensorrt(\n",
        "    context=context,\n",
        "    host_inputs=inputs,\n",
        "    input_binding_idxs=input_binding_idxs,\n",
        "    output_binding_idxs=output_binding_idxs,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "measure_accuracy(infer=infer_trt, int64=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJZMf1v7nT7B"
      },
      "source": [
        "Latency measures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS09tWaunT7B",
        "outputId": "8edd977e-74ce-45a9-b21a-7b51a0335562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TensorRT (INT-8)] mean=14.63ms, sd=0.33ms, min=13.94ms, max=17.54ms, median=14.58ms, 95p=14.76ms, 99p=15.38ms\n"
          ]
        }
      ],
      "source": [
        "time_buffer = list()\n",
        "for _ in range(100):\n",
        "    with track_infer_time(time_buffer):\n",
        "        _ = infer_tensorrt(\n",
        "            context=context,\n",
        "            host_inputs=input_np,\n",
        "            input_binding_idxs=input_binding_idxs,\n",
        "            output_binding_idxs=output_binding_idxs,\n",
        "            stream=stream,\n",
        "        )\n",
        "\n",
        "print_timings(name=\"TensorRT (INT-8)\", timings=time_buffer)\n",
        "del engine, context  # delete all tensorrt objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "R3ZfjVewnT7B"
      },
      "source": [
        "## Method 2: use a dedicated QDQ model\n",
        "\n",
        "In method 2, the idea is to take the source code of a specific model and add manually in the source code `QDQ` nodes. That way, quantization will work out of the box for this architecture.\n",
        "We have started with `QDQRoberta` a quantization compliant `Roberta` model.\n",
        "\n",
        "To adapt to another architecture, one need to:\n",
        "\n",
        "* replace linear layers with their quantized version\n",
        "* replace operations not supported out of the box by `TensorRT` by a similar code supporting the operation.\n",
        "\n",
        "> concrete examples on `Roberta` architecture: in HF library, there is a `cumsum` in the position embedding generation. Something very simple. It takes as input an integer tensor and output an integer tensor. It happens that the `cumsum` operator from TensorRT supports float but not integer (https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md). It leads to a crash during the model conversion with a strange error message. Converting the input to float tensor fix the issue. Not complex, but requires some knowledge.\n",
        "\n",
        "The process below is a bit simpler than the method 1:\n",
        "\n",
        "* Calibrate\n",
        "* Quantization Aware training (QAT)\n",
        "\n",
        "> there are many ways to get a QDQ model, you can modify Pytorch source code like here, patch ONNX graph (this approach is used at Microsoft for instance) or leverage the new FX Pytorch interface. Modifying the source code is the most straight forward so we choosed to do it that way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YnaRWcr6nT7C"
      },
      "source": [
        "### Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sdma91TOnT7C",
        "outputId": "9d828164-a2d0-4a4a-ae7e-0b2f7da7c138",
        "colab": {
          "referenced_widgets": [
            "b3d2a85cc8674f6db9ccc11bdb52794a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3d2a85cc8674f6db9ccc11bdb52794a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "input_desc = QuantDescriptor(num_bits=8, calib_method=\"histogram\")\n",
        "# below we do per-channel quantization for weights, set axis to None to get a per tensor calibration\n",
        "weight_desc = QuantDescriptor(num_bits=8, axis=(0,))\n",
        "quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)\n",
        "quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc)\n",
        "\n",
        "# keep it on CPU\n",
        "model_roberta: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=num_labels\n",
        ")\n",
        "model_roberta.save_pretrained(\"roberta-untrained-quantized\")\n",
        "del model_roberta\n",
        "\n",
        "model_roberta_q: PreTrainedModel = QDQRobertaForSequenceClassification.from_pretrained(\"roberta-untrained-quantized\")\n",
        "model_roberta_q = calibrate(model=model_roberta_q, encoded_dataset=encoded_dataset)\n",
        "model_roberta_q.save_pretrained(\"roberta-untrained-quantized\")\n",
        "del model_roberta_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BDyjk5v0nT7C"
      },
      "source": [
        "### Quantization Aware Training (QAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KrLnQzbUnT7C",
        "outputId": "1ef3a380-8752-4414-ca8e-1daf07b23290"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|trainer.py:437] 2021-12-09 22:10:02,417 >> Using amp half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7446, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08}\n",
            "{'eval_loss': 0.5072791576385498, 'eval_accuracy': 0.8059093224656139, 'eval_runtime': 47.7062, 'eval_samples_per_second': 205.738, 'eval_steps_per_second': 3.228, 'epoch': 0.08}\n",
            "{'loss': 0.5424, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16}\n",
            "{'eval_loss': 0.4495479464530945, 'eval_accuracy': 0.8264900662251655, 'eval_runtime': 46.5832, 'eval_samples_per_second': 210.698, 'eval_steps_per_second': 3.306, 'epoch': 0.16}\n",
            "{'loss': 0.5071, 'learning_rate': 7.558670143415907e-06, 'epoch': 0.24}\n",
            "{'eval_loss': 0.4541535973548889, 'eval_accuracy': 0.8242485990830362, 'eval_runtime': 50.0162, 'eval_samples_per_second': 196.236, 'eval_steps_per_second': 3.079, 'epoch': 0.24}\n",
            "{'loss': 0.4854, 'learning_rate': 6.743807040417211e-06, 'epoch': 0.33}\n",
            "{'eval_loss': 0.4110897183418274, 'eval_accuracy': 0.8425878757004585, 'eval_runtime': 47.5433, 'eval_samples_per_second': 206.443, 'eval_steps_per_second': 3.239, 'epoch': 0.33}\n",
            "{'loss': 0.4656, 'learning_rate': 5.929758800521513e-06, 'epoch': 0.41}\n",
            "{'eval_loss': 0.4083874821662903, 'eval_accuracy': 0.8410596026490066, 'eval_runtime': 47.486, 'eval_samples_per_second': 206.693, 'eval_steps_per_second': 3.243, 'epoch': 0.41}\n",
            "{'loss': 0.4547, 'learning_rate': 5.1148956975228174e-06, 'epoch': 0.49}\n",
            "{'eval_loss': 0.40900033712387085, 'eval_accuracy': 0.8411614875191035, 'eval_runtime': 48.7052, 'eval_samples_per_second': 201.518, 'eval_steps_per_second': 3.162, 'epoch': 0.49}\n",
            "{'loss': 0.4503, 'learning_rate': 4.30003259452412e-06, 'epoch': 0.57}\n",
            "{'eval_loss': 0.391275018453598, 'eval_accuracy': 0.8503311258278146, 'eval_runtime': 47.4931, 'eval_samples_per_second': 206.662, 'eval_steps_per_second': 3.243, 'epoch': 0.57}\n",
            "{'loss': 0.4433, 'learning_rate': 3.4851694915254244e-06, 'epoch': 0.65}\n",
            "{'eval_loss': 0.3878655731678009, 'eval_accuracy': 0.851044319918492, 'eval_runtime': 49.1368, 'eval_samples_per_second': 199.748, 'eval_steps_per_second': 3.134, 'epoch': 0.65}\n",
            "{'loss': 0.4323, 'learning_rate': 2.6711212516297265e-06, 'epoch': 0.73}\n",
            "{'eval_loss': 0.38398584723472595, 'eval_accuracy': 0.8541008660213958, 'eval_runtime': 48.2456, 'eval_samples_per_second': 203.438, 'eval_steps_per_second': 3.192, 'epoch': 0.73}\n",
            "{'loss': 0.4238, 'learning_rate': 1.8570730117340288e-06, 'epoch': 0.81}\n",
            "{'eval_loss': 0.38184261322021484, 'eval_accuracy': 0.8535914416709118, 'eval_runtime': 47.2522, 'eval_samples_per_second': 207.715, 'eval_steps_per_second': 3.259, 'epoch': 0.81}\n",
            "{'loss': 0.4253, 'learning_rate': 1.0422099087353325e-06, 'epoch': 0.9}\n",
            "{'eval_loss': 0.37562793493270874, 'eval_accuracy': 0.856953642384106, 'eval_runtime': 47.5902, 'eval_samples_per_second': 206.24, 'eval_steps_per_second': 3.236, 'epoch': 0.9}\n",
            "{'loss': 0.4248, 'learning_rate': 2.2734680573663624e-07, 'epoch': 0.98}\n",
            "{'eval_loss': 0.37268802523612976, 'eval_accuracy': 0.8588894549159449, 'eval_runtime': 48.7185, 'eval_samples_per_second': 201.463, 'eval_steps_per_second': 3.161, 'epoch': 0.98}\n",
            "{'train_runtime': 5103.4383, 'train_samples_per_second': 76.949, 'train_steps_per_second': 2.405, 'train_loss': 0.4821581125570245, 'epoch': 1.0}\n",
            "{'eval_loss': 0.37268802523612976, 'eval_accuracy': 0.8588894549159449, 'eval_runtime': 47.6115, 'eval_samples_per_second': 206.148, 'eval_steps_per_second': 3.235, 'epoch': 1.0}\n",
            "{'eval_loss': 0.37268802523612976, 'eval_accuracy': 0.8588894549159449, 'eval_runtime': 47.6115, 'eval_samples_per_second': 206.148, 'eval_steps_per_second': 3.235, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "model_roberta_q: PreTrainedModel = QDQRobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-untrained-quantized\", num_labels=num_labels\n",
        ")\n",
        "model_roberta_q = model_roberta_q.cuda()\n",
        "\n",
        "args.learning_rate = 1e-5\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_roberta_q,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[validation_key],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "transformers.logging.set_verbosity_error()\n",
        "trainer.train()\n",
        "print(trainer.evaluate())\n",
        "model_roberta_q.save_pretrained(\"roberta-trained-quantized\")\n",
        "del model_roberta_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "u5rIG-QDnT7D"
      },
      "source": [
        "### Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Fu2KNInT7D"
      },
      "source": [
        "#### Export a `QDQ Pytorch` model on `ONNX`, we need to enable fake quantization mode from Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ptAk3vIMnT7D",
        "outputId": "391c9c9e-85e2-4fba-eb90-8a5e01a54c07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  inputs, amax.item() / bound, 0,\n",
            "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n"
          ]
        }
      ],
      "source": [
        "model_roberta_q: PreTrainedModel = QDQRobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-trained-quantized\", num_labels=num_labels\n",
        ")\n",
        "model_roberta_q = model_roberta_q.cuda()\n",
        "\n",
        "data = encoded_dataset[\"train\"][1:3]\n",
        "input_torch = convert_tensor(data, output=\"torch\")\n",
        "\n",
        "from pytorch_quantization.nn import TensorQuantizer\n",
        "\n",
        "TensorQuantizer.use_fb_fake_quant = True\n",
        "convert_to_onnx(model_pytorch=model_roberta_q, output_path=\"roberta_q.onnx\", inputs_pytorch=input_torch, opset=13)\n",
        "TensorQuantizer.use_fb_fake_quant = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HPycNY7nT7E"
      },
      "source": [
        "#### Convert `ONNX` graph to `TensorRT` engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XrUSNkGEnT7E"
      },
      "outputs": [],
      "source": [
        "engine = build_engine(\n",
        "    runtime=runtime,\n",
        "    onnx_file_path=\"roberta_q.onnx\",\n",
        "    logger=trt_logger,\n",
        "    min_shape=(1, max_seq_len),\n",
        "    optimal_shape=(batch_size, max_seq_len),\n",
        "    max_shape=(batch_size, max_seq_len),\n",
        "    workspace_size=10000 * 1024 * 1024,\n",
        "    fp16=False,\n",
        "    int8=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "id": "Xp-UgrctnT7E"
      },
      "outputs": [],
      "source": [
        "# same conversion from the terminal\n",
        "#!/usr/src/tensorrt/bin/trtexec --onnx=roberta_q.onnx --shapes=input_ids:32x256,attention_mask:32x256 --int8 --workspace=10000 --saveEngine=\"test.plan\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8hEdjhknT7F"
      },
      "source": [
        "#### Prepare input and output buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wnq6bOGOnT7F"
      },
      "outputs": [],
      "source": [
        "stream: Stream = pycuda.driver.Stream()\n",
        "context: IExecutionContext = engine.create_execution_context()\n",
        "context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle)\n",
        "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index)  # type: List[int], List[int]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DNel-XrtnT7F"
      },
      "outputs": [],
      "source": [
        "data = encoded_dataset[\"train\"][0:batch_size]\n",
        "input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\")\n",
        "input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4OuFs0onT7F"
      },
      "source": [
        "#### Inference on `TensorRT`\n",
        "\n",
        "We first check that inference is working correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eBWMA2VQnT7F",
        "outputId": "2c744781-5167-47c3-80d5-5a28ab7ceac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[-0.51485693,  1.7172506 , -1.5262733 ],\n",
            "       [ 2.353083  , -1.1611496 , -1.5365003 ],\n",
            "       [ 1.7413325 , -0.5790743 , -1.659783  ],\n",
            "       [ 1.564936  , -0.49288243, -1.3157034 ],\n",
            "       [ 1.625774  , -0.06960616, -2.3415694 ],\n",
            "       [ 3.4986691 , -1.4058641 , -2.909094  ],\n",
            "       [-2.8577392 ,  2.3696911 ,  0.9519228 ],\n",
            "       [ 3.3248267 , -1.3577703 , -2.7853382 ],\n",
            "       [ 0.24115235, -1.2206222 ,  1.9764783 ],\n",
            "       [-2.1684752 , -0.5929435 ,  4.1980004 ],\n",
            "       [ 2.7209766 , -0.85320175, -2.54238   ],\n",
            "       [-1.4474616 , -0.5539231 ,  3.543574  ],\n",
            "       [-2.4900246 ,  2.5807233 ,  0.29105982],\n",
            "       [-2.5218582 ,  2.4110076 ,  0.4147416 ],\n",
            "       [-0.17686448,  0.19154471,  0.5593225 ],\n",
            "       [ 2.7820387 , -0.92807496, -2.466081  ],\n",
            "       [ 3.279974  , -1.1566027 , -3.082859  ],\n",
            "       [-2.0141928 , -1.5038209 ,  4.759576  ],\n",
            "       [-2.8134325 , -0.09846646,  4.3754187 ],\n",
            "       [-2.4098513 ,  3.2899659 , -1.0759622 ],\n",
            "       [ 3.3163776 , -1.0768431 , -3.2254322 ],\n",
            "       [ 1.8269717 ,  0.69882363, -3.383636  ],\n",
            "       [-2.5522573 , -0.6264023 ,  4.348268  ],\n",
            "       [ 3.4143322 , -1.0857687 , -3.3268075 ],\n",
            "       [ 3.418143  , -1.5472901 , -2.7069504 ],\n",
            "       [-0.9896777 ,  0.2267024 ,  1.1920347 ],\n",
            "       [-1.9947617 ,  0.58624893,  2.7530055 ],\n",
            "       [-2.328186  ,  3.3224452 , -1.4264905 ],\n",
            "       [-2.8432767 ,  0.7639467 ,  3.400511  ],\n",
            "       [ 2.8967564 , -0.8297742 , -3.1207962 ],\n",
            "       [ 3.0185122 , -1.3341582 , -2.4004488 ],\n",
            "       [ 3.050612  , -1.0362974 , -2.8093874 ]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "tensorrt_output = infer_tensorrt(\n",
        "    context=context,\n",
        "    host_inputs=input_np,\n",
        "    input_binding_idxs=input_binding_idxs,\n",
        "    output_binding_idxs=output_binding_idxs,\n",
        "    stream=stream,\n",
        ")\n",
        "print(tensorrt_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdjJU95NnT7G"
      },
      "source": [
        "Measure of the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZrnAXgpnT7G",
        "outputId": "8b4086a3-5381-45a1-d7bd-a667cdadda7d",
        "colab": {
          "referenced_widgets": [
            "39a7ce7f07b34049a82b0721dcb0322e"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39a7ce7f07b34049a82b0721dcb0322e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/307 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.8577687213448802"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "infer_trt = lambda inputs: infer_tensorrt(\n",
        "    context=context,\n",
        "    host_inputs=inputs,\n",
        "    input_binding_idxs=input_binding_idxs,\n",
        "    output_binding_idxs=output_binding_idxs,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "measure_accuracy(infer=infer_trt, int64=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOMQFqDmnT7G"
      },
      "source": [
        "Latency measures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OBLMZp3nT7H",
        "outputId": "f1b26d0c-5a0a-4748-cc10-c364e6f7bb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TensorRT (INT-8)] mean=15.52ms, sd=0.62ms, min=14.49ms, max=17.17ms, median=15.15ms, 95p=16.67ms, 99p=17.03ms\n"
          ]
        }
      ],
      "source": [
        "time_buffer = list()\n",
        "for _ in range(100):\n",
        "    with track_infer_time(time_buffer):\n",
        "        _ = infer_tensorrt(\n",
        "            context=context,\n",
        "            host_inputs=input_np,\n",
        "            input_binding_idxs=input_binding_idxs,\n",
        "            output_binding_idxs=output_binding_idxs,\n",
        "            stream=stream,\n",
        "        )\n",
        "\n",
        "print_timings(name=\"TensorRT (INT-8)\", timings=time_buffer)\n",
        "del engine, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgJq46u5nT7H"
      },
      "source": [
        "## Pytorch baseline\n",
        "\n",
        "### Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bW49l3K5nT7H",
        "outputId": "433ef558-5dc7-4c47-a26d-faaa5a5d6821"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|trainer.py:437] 2021-12-09 23:38:27,822 >> Using amp half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6612, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08}\n",
            "{'eval_loss': 0.4690713882446289, 'eval_accuracy': 0.8217014773306164, 'eval_runtime': 19.0461, 'eval_samples_per_second': 515.328, 'eval_steps_per_second': 8.086, 'epoch': 0.08}\n",
            "{'loss': 0.4984, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16}\n",
            "{'eval_loss': 0.4219346344470978, 'eval_accuracy': 0.835048395313296, 'eval_runtime': 18.9872, 'eval_samples_per_second': 516.928, 'eval_steps_per_second': 8.111, 'epoch': 0.16}\n",
            "{'loss': 0.4664, 'learning_rate': 7.558670143415907e-06, 'epoch': 0.24}\n",
            "{'eval_loss': 0.4248501658439636, 'eval_accuracy': 0.835048395313296, 'eval_runtime': 18.5315, 'eval_samples_per_second': 529.639, 'eval_steps_per_second': 8.31, 'epoch': 0.24}\n",
            "{'loss': 0.4471, 'learning_rate': 6.743807040417211e-06, 'epoch': 0.33}\n",
            "{'eval_loss': 0.3851495087146759, 'eval_accuracy': 0.853998981151299, 'eval_runtime': 18.5175, 'eval_samples_per_second': 530.039, 'eval_steps_per_second': 8.316, 'epoch': 0.33}\n",
            "{'loss': 0.4291, 'learning_rate': 5.9289439374185145e-06, 'epoch': 0.41}\n",
            "{'eval_loss': 0.3886096775531769, 'eval_accuracy': 0.8529801324503311, 'eval_runtime': 19.2048, 'eval_samples_per_second': 511.07, 'eval_steps_per_second': 8.019, 'epoch': 0.41}\n",
            "{'loss': 0.4198, 'learning_rate': 5.114080834419818e-06, 'epoch': 0.49}\n",
            "{'eval_loss': 0.3939703404903412, 'eval_accuracy': 0.8499235863474274, 'eval_runtime': 19.079, 'eval_samples_per_second': 514.44, 'eval_steps_per_second': 8.072, 'epoch': 0.49}\n",
            "{'loss': 0.417, 'learning_rate': 4.30003259452412e-06, 'epoch': 0.57}\n",
            "{'eval_loss': 0.36645272374153137, 'eval_accuracy': 0.8580743759551707, 'eval_runtime': 18.5783, 'eval_samples_per_second': 528.305, 'eval_steps_per_second': 8.289, 'epoch': 0.57}\n",
            "{'loss': 0.4117, 'learning_rate': 3.4851694915254244e-06, 'epoch': 0.65}\n",
            "{'eval_loss': 0.3587413430213928, 'eval_accuracy': 0.860825267447784, 'eval_runtime': 18.6272, 'eval_samples_per_second': 526.919, 'eval_steps_per_second': 8.267, 'epoch': 0.65}\n",
            "{'loss': 0.4014, 'learning_rate': 2.670306388526728e-06, 'epoch': 0.73}\n",
            "{'eval_loss': 0.3596762418746948, 'eval_accuracy': 0.8618441161487519, 'eval_runtime': 18.5056, 'eval_samples_per_second': 530.379, 'eval_steps_per_second': 8.322, 'epoch': 0.73}\n",
            "{'loss': 0.394, 'learning_rate': 1.8554432855280313e-06, 'epoch': 0.81}\n",
            "{'eval_loss': 0.35547441244125366, 'eval_accuracy': 0.8645950076413652, 'eval_runtime': 18.5169, 'eval_samples_per_second': 530.056, 'eval_steps_per_second': 8.317, 'epoch': 0.81}\n",
            "{'loss': 0.3967, 'learning_rate': 1.0422099087353325e-06, 'epoch': 0.9}\n",
            "{'eval_loss': 0.350873202085495, 'eval_accuracy': 0.8677534386143657, 'eval_runtime': 18.5202, 'eval_samples_per_second': 529.963, 'eval_steps_per_second': 8.315, 'epoch': 0.9}\n",
            "{'loss': 0.3971, 'learning_rate': 2.2734680573663624e-07, 'epoch': 0.98}\n",
            "{'eval_loss': 0.350557416677475, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 18.4974, 'eval_samples_per_second': 530.616, 'eval_steps_per_second': 8.326, 'epoch': 0.98}\n",
            "{'train_runtime': 2679.9953, 'train_samples_per_second': 146.531, 'train_steps_per_second': 4.579, 'train_loss': 0.44397361524101964, 'epoch': 1.0}\n",
            "{'eval_loss': 0.350873202085495, 'eval_accuracy': 0.8677534386143657, 'eval_runtime': 18.5374, 'eval_samples_per_second': 529.471, 'eval_steps_per_second': 8.308, 'epoch': 1.0}\n",
            "{'eval_loss': 0.350873202085495, 'eval_accuracy': 0.8677534386143657, 'eval_runtime': 18.5374, 'eval_samples_per_second': 529.471, 'eval_steps_per_second': 8.308, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "model_roberta: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=num_labels\n",
        ")\n",
        "model_roberta = model_roberta.cuda()\n",
        "\n",
        "args.learning_rate = 1e-5\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_roberta,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[validation_key],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "transformers.logging.set_verbosity_error()\n",
        "trainer.train()\n",
        "print(trainer.evaluate())\n",
        "# {'eval_loss': 0.3559744358062744, 'eval_accuracy': 0.8655119714722364, 'eval_runtime': 19.6678, 'eval_samples_per_second': 499.04, 'eval_steps_per_second': 7.83, 'epoch': 0.98}\n",
        "trainer.save_model(\"roberta-baseline\")\n",
        "del model_roberta\n",
        "del trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0cc5NQDnT7H"
      },
      "source": [
        "### GPU execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7JjM9qdnT7I"
      },
      "source": [
        "To finish, we will measure vanilla Pytorch inference on both FP32 and FP16 precision, it will be our baseline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyErt3jNnT7I",
        "outputId": "0956f771-be7e-4f25-855e-6128790b1fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pytorch (FP32)] mean=79.48ms, sd=0.84ms, min=78.64ms, max=82.72ms, median=79.26ms, 95p=81.66ms, 99p=82.47ms\n"
          ]
        }
      ],
      "source": [
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-baseline\", num_labels=num_labels)\n",
        "baseline_model = baseline_model.cuda()\n",
        "baseline_model = baseline_model.eval()\n",
        "\n",
        "data = encoded_dataset[\"train\"][0:batch_size]\n",
        "input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for _ in range(30):\n",
        "        _ = baseline_model(**input_torch)\n",
        "        torch.cuda.synchronize()\n",
        "    time_buffer = list()\n",
        "    for _ in range(100):\n",
        "        with track_infer_time(time_buffer):\n",
        "            _ = baseline_model(**input_torch)\n",
        "            torch.cuda.synchronize()\n",
        "print_timings(name=\"Pytorch (FP32)\", timings=time_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRtPwv14nT7I",
        "outputId": "bb71e3cd-af85-4a79-b010-065643d4c153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pytorch (FP16)] mean=58.02ms, sd=0.59ms, min=57.46ms, max=60.90ms, median=57.80ms, 95p=59.52ms, 99p=60.49ms\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "    with torch.cuda.amp.autocast():\n",
        "        for _ in range(30):\n",
        "            _ = baseline_model(**input_torch)\n",
        "            torch.cuda.synchronize()\n",
        "        time_buffer = []\n",
        "        for _ in range(100):\n",
        "            with track_infer_time(time_buffer):\n",
        "                _ = baseline_model(**input_torch)\n",
        "                torch.cuda.synchronize()\n",
        "print_timings(name=\"Pytorch (FP16)\", timings=time_buffer)\n",
        "del baseline_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc6CyEmHnT7I"
      },
      "source": [
        "### CPU execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_vUihcpnT7J",
        "outputId": "df8d03a9-bff6-4a45-b2f6-cb48ec8ee34e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pytorch (FP32) - CPU] mean=3999.80ms, sd=27.86ms, min=3946.75ms, max=4042.56ms, median=4000.13ms, 95p=4037.90ms, 99p=4041.63ms\n"
          ]
        }
      ],
      "source": [
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-baseline\", num_labels=num_labels)\n",
        "baseline_model = baseline_model.eval()\n",
        "data = encoded_dataset[\"train\"][0:batch_size]\n",
        "input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\")\n",
        "input_torch_cpu = {k: v.to(\"cpu\") for k, v in input_torch.items()}\n",
        "\n",
        "import os\n",
        "\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for _ in range(3):\n",
        "        _ = baseline_model(**input_torch_cpu)\n",
        "        torch.cuda.synchronize()\n",
        "    time_buffer = list()\n",
        "    for _ in range(10):\n",
        "        with track_infer_time(time_buffer):\n",
        "            _ = baseline_model(**input_torch_cpu)\n",
        "            torch.cuda.synchronize()\n",
        "print_timings(name=\"Pytorch (FP32) - CPU\", timings=time_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu4Q6VUFnT7J",
        "outputId": "8e9cc858-ff56-4957-c4dc-07c50f9570c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pytorch (FP16) - CPU] mean=4005.57ms, sd=46.34ms, min=3922.37ms, max=4095.30ms, median=4010.20ms, 95p=4071.75ms, 99p=4090.59ms\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "    with torch.cuda.amp.autocast():\n",
        "        for _ in range(3):\n",
        "            _ = baseline_model(**input_torch_cpu)\n",
        "            torch.cuda.synchronize()\n",
        "        time_buffer = []\n",
        "        for _ in range(10):\n",
        "            with track_infer_time(time_buffer):\n",
        "                _ = baseline_model(**input_torch_cpu)\n",
        "                torch.cuda.synchronize()\n",
        "print_timings(name=\"Pytorch (FP16) - CPU\", timings=time_buffer)\n",
        "del baseline_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTj_ziuinT7J"
      },
      "source": [
        "Below, we will perform dynamic quantization on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRcVEKRXnT7K",
        "outputId": "14a25d3a-fcce-46ba-8389-42e277d78d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pytorch (INT-8) - CPU] mean=3669.84ms, sd=25.49ms, min=3633.48ms, max=3712.22ms, median=3670.07ms, 95p=3706.98ms, 99p=3711.17ms\n"
          ]
        }
      ],
      "source": [
        "quantized_baseline_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-baseline\", num_labels=num_labels)\n",
        "quantized_baseline_model = quantized_baseline_model.eval()\n",
        "quantized_baseline_model = torch.quantization.quantize_dynamic(\n",
        "    quantized_baseline_model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for _ in range(3):\n",
        "        _ = quantized_baseline_model(**input_torch_cpu)\n",
        "        torch.cuda.synchronize()\n",
        "    time_buffer = list()\n",
        "    for _ in range(10):\n",
        "        with track_infer_time(time_buffer):\n",
        "            _ = quantized_baseline_model(**input_torch_cpu)\n",
        "            torch.cuda.synchronize()\n",
        "print_timings(name=\"Pytorch (INT-8) - CPU\", timings=time_buffer)\n",
        "del quantized_baseline_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMkhVcrUnT7K"
      },
      "source": [
        "## TensorRT baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-3I75JCnT7K"
      },
      "source": [
        "Below we export a randomly initialized `Roberta` model, the purpose is to only check the performance on mixed precision (FP16, no quantization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "O_l9rDEunT7K"
      },
      "outputs": [],
      "source": [
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-baseline\", num_labels=num_labels)\n",
        "baseline_model = baseline_model.cuda()\n",
        "convert_to_onnx(baseline_model, output_path=\"baseline.onnx\", inputs_pytorch=input_torch, opset=12)\n",
        "del baseline_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g4wy6j1qnT7L",
        "outputId": "e2ee09f8-cbdb-4dbf-cfba-22ba619b8002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TensorRT (FP16)] mean=30.23ms, sd=0.49ms, min=29.73ms, max=32.67ms, median=30.06ms, 95p=30.82ms, 99p=32.38ms\n"
          ]
        }
      ],
      "source": [
        "engine = build_engine(\n",
        "    runtime=runtime,\n",
        "    onnx_file_path=\"baseline.onnx\",\n",
        "    logger=trt_logger,\n",
        "    min_shape=(batch_size, max_seq_len),\n",
        "    optimal_shape=(batch_size, max_seq_len),\n",
        "    max_shape=(batch_size, max_seq_len),\n",
        "    workspace_size=10000 * 1024 * 1024,\n",
        "    fp16=True,\n",
        "    int8=False,\n",
        ")\n",
        "stream: Stream = pycuda.driver.Stream()\n",
        "context: IExecutionContext = engine.create_execution_context()\n",
        "context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle)\n",
        "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index)  # type: List[int], List[int]\n",
        "for _ in range(30):\n",
        "    _ = infer_tensorrt(\n",
        "        context=context,\n",
        "        host_inputs=input_np,\n",
        "        input_binding_idxs=input_binding_idxs,\n",
        "        output_binding_idxs=output_binding_idxs,\n",
        "        stream=stream,\n",
        "    )\n",
        "time_buffer = list()\n",
        "for _ in range(100):\n",
        "    with track_infer_time(time_buffer):\n",
        "        _ = infer_tensorrt(\n",
        "            context=context,\n",
        "            host_inputs=input_np,\n",
        "            input_binding_idxs=input_binding_idxs,\n",
        "            output_binding_idxs=output_binding_idxs,\n",
        "            stream=stream,\n",
        "        )\n",
        "\n",
        "print_timings(name=\"TensorRT (FP16)\", timings=time_buffer)\n",
        "del engine, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZmuqSc3nT7L"
      },
      "source": [
        "## ONNX Runtime baseline\n",
        "\n",
        "ONNX Runtime is the go to inference solution from Microsoft.\n",
        "\n",
        "The recent 1.10 version of ONNX Runtime (with TensorRT support) is still a bit buggy on transformer models, that is why we use the 1.9.0 version in the measures below.\n",
        "\n",
        "As before, CPU quantization is dynamic.\n",
        "Function `create_model_for_provider` will set ONNX Runtime to use all cores available and enable any possible optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "j5Ki5rd8nT7L",
        "outputId": "405e582c-36dd-4f97-9105-c0f4a7faffd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
            "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n"
          ]
        }
      ],
      "source": [
        "from transformer_deploy.backends.ort_utils import optimize_onnx, create_model_for_provider\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "optimize_onnx(\n",
        "    onnx_path=\"baseline.onnx\",\n",
        "    onnx_optim_fp16_path=\"baseline-optimized.onnx\",\n",
        "    use_cuda=True,\n",
        ")\n",
        "onnx_model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
        "quantize_dynamic(\"baseline-optimized.onnx\", \"baseline-quantized.onnx\", weight_type=QuantType.QUInt8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcqYOFqAnT7N"
      },
      "outputs": [],
      "source": [
        "labels = [item[\"label\"] for item in encoded_dataset[validation_key]]\n",
        "data = encoded_dataset[validation_key][0:batch_size]\n",
        "inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\")\n",
        "for k, v in inputs_onnx.items():\n",
        "    inputs_onnx[k] = v.astype(np.int64)\n",
        "\n",
        "model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
        "output = model.run(None, inputs_onnx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-AsDSShnT7Q",
        "outputId": "137456f9-1f29-4be5-9e3a-768a1975058e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ONNX Runtime GPU (FP32)] mean=74.48ms, sd=0.55ms, min=73.87ms, max=76.61ms, median=74.36ms, 95p=75.89ms, 99p=76.34ms\n",
            "[ONNX Runtime GPU (FP16)] mean=33.50ms, sd=0.62ms, min=32.90ms, max=37.58ms, median=33.39ms, 95p=34.42ms, 99p=35.47ms\n",
            "[ONNX Runtime CPU (FP32)] mean=3767.02ms, sd=32.02ms, min=3720.72ms, max=3831.88ms, median=3766.35ms, 95p=3816.04ms, 99p=3828.71ms\n",
            "[ONNX Runtime CPU (FP16)] mean=4607.67ms, sd=121.41ms, min=4513.24ms, max=4950.20ms, median=4573.18ms, 95p=4822.23ms, 99p=4924.61ms\n",
            "[ONNX Runtime CPU (INT-8)] mean=3712.67ms, sd=45.19ms, min=3656.30ms, max=3827.99ms, median=3709.21ms, 95p=3788.00ms, 99p=3819.99ms\n"
          ]
        }
      ],
      "source": [
        "data = encoded_dataset[\"train\"][0:batch_size]\n",
        "inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\")\n",
        "for k, v in inputs_onnx.items():\n",
        "    inputs_onnx[k] = v.astype(np.int64)\n",
        "\n",
        "for provider, model_path, benchmark_name, warmup, nb_inference in [\n",
        "    (\"CUDAExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime GPU (FP32)\", 10, 100),\n",
        "    (\"CUDAExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime GPU (FP16)\", 10, 100),\n",
        "    (\"CPUExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime CPU (FP32)\", 3, 10),\n",
        "    (\"CPUExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime CPU (FP16)\", 3, 10),\n",
        "    (\"CPUExecutionProvider\", \"baseline-quantized.onnx\", \"ONNX Runtime CPU (INT-8)\", 3, 10),\n",
        "]:\n",
        "    model = create_model_for_provider(path=model_path, provider_to_use=provider)\n",
        "    for _ in range(warmup):\n",
        "        _ = model.run(None, inputs_onnx)\n",
        "    time_buffer = []\n",
        "    for _ in range(nb_inference):\n",
        "        with track_infer_time(time_buffer):\n",
        "            _ = model.run(None, inputs_onnx)\n",
        "    print_timings(name=benchmark_name, timings=time_buffer)\n",
        "    del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gldHSJornT7Q"
      },
      "source": [
        "Measure of the accuracy with ONNX Runtime engine and CUDA provider:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcw1JpS-nT7Q",
        "outputId": "0e4a26c8-bd9c-488a-d907-81bfea820a56",
        "colab": {
          "referenced_widgets": [
            "ee60729e561d492f82a7db2c93fc44ac"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee60729e561d492f82a7db2c93fc44ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/307 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.8678553234844626"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = create_model_for_provider(path=\"baseline.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
        "infer_ort = lambda tokens: model.run(None, tokens)\n",
        "measure_accuracy(infer=infer_ort, int64=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzXhP9J9nT7R",
        "outputId": "7f1e933e-6c2a-4b24-8ca6-40148ccdb15a",
        "colab": {
          "referenced_widgets": [
            "4c78489324dc4d8f9206543d59061f9e"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c78489324dc4d8f9206543d59061f9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/307 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.8675496688741722"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
        "infer_ort = lambda tokens: model.run(None, tokens)\n",
        "measure_accuracy(infer=infer_ort, int64=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "whPRbBNbIrIl",
        "n9qywopnIrJH",
        "7k8ge1L1IrJk"
      ],
      "name": "Copie de Text Classification on GLUE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}