{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlyabhilash/Transformers_with_NLP/blob/main/DialoGPT/DialoGPT_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EnUCcmutUj8",
        "outputId": "1aea48fb-eb7d-4448-ab99-441f76499d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov  8 09:51:36 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KmHwJqptSnO",
        "outputId": "c5ee89cb-78ad-4f10-8505-7749986c1c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.6MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.3)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.7)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=618f2873190b502c0db83e2bc079629e979b69e536e9fe131c09eceedad90551\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sentencepiece, sacremoses, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzIGSWmQtjJC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "41fee34b-9b04-49d0-dbd9-44751ddb19c0"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# import huggingface transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW, WarmupLinearSchedule"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYKxjlBTt0wl"
      },
      "source": [
        "def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "    \"\"\"\n",
        "    # batch support!\n",
        "    if top_k > 0:\n",
        "        values, _ = torch.topk(logits, top_k)\n",
        "        min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])\n",
        "        logits = torch.where(logits < min_values, \n",
        "                             torch.ones_like(logits, dtype=logits.dtype) * -float('Inf'), \n",
        "                             logits)\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        \n",
        "        sorted_logits = sorted_logits.masked_fill_(sorted_indices_to_remove, filter_value)\n",
        "        logits = torch.zeros_like(logits).scatter(1, sorted_indices, sorted_logits)\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qoz-KEcbWgT"
      },
      "source": [
        "np.random.seed(args.seed)\n",
        "torch.random.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0cQrCEXubmN"
      },
      "source": [
        "gpt2_small_config = GPT2Config()\n",
        "gpt2_medium_config = GPT2Config(n_ctx=1024, n_embd=1024, n_layer=24, n_head=16)\n",
        "gpt2_large_config = GPT2Config(n_ctx=1024, n_embd=1280, n_layer=36, n_head=20)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagx89vDt4dR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1fd6e650-5b2a-4304-97ff-c4c1cd5dfc1c"
      },
      "source": [
        "# load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 2092545.51B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 1347896.65B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LqRxx9xwsdj",
        "outputId": "66371a1d-f6b3-4dbb-ae9c-3b1f13b6e462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# download all model weights\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/large_ft.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-08 09:51:56--  https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 351265273 (335M) [application/octet-stream]\n",
            "Saving to: ‘small_ft.pkl’\n",
            "\n",
            "small_ft.pkl          5%[>                   ]  19.49M  4.98MB/s    eta 81s    ^C\n",
            "--2019-11-08 09:52:02--  https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862954531 (823M) [application/octet-stream]\n",
            "Saving to: ‘medium_ft.pkl’\n",
            "\n",
            "medium_ft.pkl         0%[                    ]       0  --.-KB/s               ^C\n",
            "--2019-11-08 09:52:04--  https://convaisharables.blob.core.windows.net/lsp/multiref/large_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1752291179 (1.6G) [application/octet-stream]\n",
            "Saving to: ‘large_ft.pkl’\n",
            "\n",
            "large_ft.pkl          4%[                    ]  67.09M  4.58MB/s    eta 5m 34s ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KzZLsMwt6Up"
      },
      "source": [
        "# load the model\n",
        "model_size = \"medium\"\n",
        "\n",
        "if model_size == \"small\":\n",
        "    model = GPT2LMHeadModel(gpt2_small_config)\n",
        "    model.load_state_dict(torch.load(\"small_ft.pkl\"), strict=False)\n",
        "elif model_size == \"medium\":\n",
        "    model = GPT2LMHeadModel(gpt2_medium_config)\n",
        "    model.load_state_dict(torch.load(\"medium_ft.pkl\"), strict=False)\n",
        "elif model_size == \"large\":\n",
        "    model = GPT2LMHeadModel(gpt2_large_config)\n",
        "    model.load_state_dict(torch.load(\"large_ft.pkl\"), strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_UH3o1kbTGl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAVF1rpyCKw"
      },
      "source": [
        "# beg huggingface not to change this anymore\n",
        "model.lm_head.weight.data = model.transformer.wte.weight.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lecxn-gwMpH"
      },
      "source": [
        "eos = [tokenizer.encoder[\"<|endoftext|>\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIojaBXEwYeX",
        "outputId": "90cfa023-3fa0-418c-a186-c3eb1aa2e7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "past = None\n",
        "temperature = 0.9\n",
        "top_k = -1\n",
        "top_p = 0.9\n",
        "\n",
        "model.eval()\n",
        "prev_input = None\n",
        "\n",
        "while True:\n",
        "    with torch.no_grad():\n",
        "        # input and update B's utterance\n",
        "        user = input(\"User:\")\n",
        "        \n",
        "        if user == \"quit\":\n",
        "            \"stop talking!\"\n",
        "            break\n",
        "        \n",
        "        user = tokenizer.encode(user)\n",
        "        prev_input = user\n",
        "        prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
        "        _, past = model(prev_input, past=past)\n",
        "\n",
        "        prev_input = torch.LongTensor([eos]).to(device)\n",
        "    \n",
        "\n",
        "        sent = []\n",
        "        for i in range(500):\n",
        "            logits, past = model(prev_input, past=past)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            prev_input = torch.multinomial(probs, num_samples=1)\n",
        "            prev_word = prev_input.item()\n",
        "\n",
        "            if prev_word == eos[0]:\n",
        "                break\n",
        "            sent.append(prev_word)\n",
        "        \n",
        "        print(\"Bot:\", tokenizer.decode(sent))\n",
        "        prev_input = torch.LongTensor([eos]).to(device)\n",
        "        _, past = model(prev_input, past=past)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User:hello\n",
            "Bot: Hey, how are you?\n",
            "User:I am good\n",
            "Bot: That's good. Are you sure?\n",
            "User:yes, i am pretty good\n",
            "Bot: That's good.\n",
            "User:quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4TeIw7ypL-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}