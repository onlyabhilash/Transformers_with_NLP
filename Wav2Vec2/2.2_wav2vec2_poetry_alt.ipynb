{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9MDxkhhWmvH"
      },
      "source": [
        "# NOTE: \n",
        "\n",
        "This is an alternative and in my view better approach to transcribing long audio clips. The code is taken from Lysandre Jik's (of Hugging Face) response to an issue on [Github](https://github.com/huggingface/transformers/issues/10366).\n",
        "\n",
        "Unlike my original version in notebook2.0, the code here doesn't need you to manually split up your audio files. Just use Librosa to stream a specific chunk of the audio file to transcribe one at a time.\n",
        "\n",
        "I kept both versions in the repo for reference. See [notebook2.0](https://github.com/chuachinhon/wav2vec2_transformers/blob/main/notebooks/2.0_wav2vec2_poetry.ipynb) as well for my original approach to this problem.\n",
        "\n",
        "\n",
        "# TRANSCRIBING LONG AUDIO FILES WITH WAV2VEC2 - ALT VERSION\n",
        "\n",
        "\n",
        "## REFERENCES\n",
        "\n",
        "- Documentation of [Hugging Face's implementation of Wav2Vec2](https://huggingface.co/transformers/model_doc/wav2vec2.html)\n",
        "\n",
        "- Hosted inference [API on Hugging Face](https://huggingface.co/facebook/wav2vec2-base-960h)\n",
        "\n",
        "- Paper on [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)\n",
        "\n",
        "\n",
        "## RESULTS\n",
        "\n",
        "- The output text files of the two longer trials can be downloaded [here](https://www.dropbox.com/s/aevfd7f6mwlk7ru/amanda_gorman_alt.txt?dl=0)\n",
        "\n",
        "\n",
        "## REQUIREMENTS\n",
        "\n",
        "- [transformers](https://pypi.org/project/transformers/) >= 4.3\n",
        "- [librosa](https://pypi.org/project/librosa/)\n",
        "- if you want to use your own audio clips, make sure to downsample them to 16kHz as the Wav2Vec2 model used here was pretrained and fine-tuned on 16kHz sampled speech audio. I used [Audacity](https://www.audacityteam.org/) to split up the audio files in this repo.\n",
        "\n",
        "\n",
        "## MODELS\n",
        "\n",
        "- There are several versions of the Wav2Vec2 model on Hugging Face's model hub. Check them out [here](https://huggingface.co/models?search=wav2ve).\n",
        "\n",
        "- For this notebook, I switched to the [wav2vec2-large-960h-lv60-self](https://huggingface.co/facebook/wav2vec2-base-960h) model. In earlier notebooks, I used the base version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhQB2T25WmvR"
      },
      "source": [
        "# 1. TRANSCRIBE POETRY RECITAL\n",
        "\n",
        "Source: [Amanda Gorman's](https://twitter.com/TheAmandaGorman) [evocative poem](https://www.youtube.com/watch?v=LZ055ilIiN4) at the inauguration of Joe Biden on Jan 20, 2021.\n",
        "\n",
        "Length: 5 minute 34s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1dL-TWaiL2_"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps_SRu2gWmvU"
      },
      "source": [
        "## 1.1 DEFINE FUNCTION TO TRANSCRIBE AUDIO CLIP IN 20-SECOND CHUNKS\n",
        "\n",
        "You can change the \"block_length\" parameter to any value, technically speaking. But anything above a 60s block length results in considerable out-of-memory issues. 20/30-second blocks seem to make the most sense to me.\n",
        "\n",
        "I'm setting block_length to 20s in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "084YJhhjlnfo"
      },
      "outputs": [],
      "source": [
        "# function adapted via: https://github.com/huggingface/transformers/issues/10366\n",
        "\n",
        "def asr_transcript(tokenizer, model, audio_file):\n",
        "    transcript = \"\"\n",
        "\n",
        "    # Stream over 20 seconds chunks\n",
        "    stream = librosa.stream(\n",
        "        audio_file, block_length=20, frame_length=16000, hop_length=16000\n",
        "    )\n",
        "\n",
        "    for speech in stream:\n",
        "        if len(speech.shape) > 1:\n",
        "            speech = speech[:, 0] + speech[:, 1]\n",
        "\n",
        "        input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = tokenizer.decode(predicted_ids[0])\n",
        "        transcript += transcription.lower() + \" \"\n",
        "        \n",
        "    return transcript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReoFAAaYWmvW"
      },
      "source": [
        "## 1.2 LOAD CHOICE OF MODEL-TOKENIZER, AUDIO FILE AND CHECK RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPeqhyGaWmvX"
      },
      "outputs": [],
      "source": [
        "#load tokenizer and pre-trained model\n",
        "tokenizer_transcribe = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "\n",
        "model_transcribe = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "\n",
        "audio_file = \"../audio/amanda_gorman.flac\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRq22w9NWmvY",
        "outputId": "33c0e50f-01b7-41bb-f0a4-268a6c7f6618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 43s, sys: 24.4 s, total: 9min 8s\n",
            "Wall time: 2min 50s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "poet = asr_transcript(tokenizer_transcribe, model_transcribe, audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j11Q2cbJWmva",
        "outputId": "520220d3-e4f6-48e8-f780-b0ca0654e53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mister president doctor byden madam vice president mister mhof americans and the world when day comes we ask ourselves where can we find light in this never ending shade the loss we carry a sea we must wad we've braved the belly of the beast we've learned that quiet isn't always peace in the norms and notions of what just is isn't always just is and yet the n is ours before we knew it somehow we do it somehow we've weathered and witnessed a nation that isn't broken but simply unfinished we the successors of a country and a time were a skinny black girl descended from slaves and raised by a single mother can dream of becoming president only to find herself reciting for one and yes we are far from polished far from pestine but that doesn't mean we are striving to form a union that is perfect we are striving to forge our union with purpose to compose a country committed to all cultures colors characters and conditions of man and so we lift our gaze not to what stands between us but what stands for us we close the divide because we know to put our future first we must first put our differences aside we lay down our arms so we can reach out our arms to one and nether we seek harm to none and harmony for all let the globe if nothing else say this is true thet even as we grieved we grew that even as we hurt we hoped that even as we tired we tried that will for ever be tied together victorious not because we will never again know defeat but because we will never again so division scripture tells us to invision that everyone shall sit under their own vine and fig tree and no one shall make them afraid if we're to live up to our own time the victory won't lie in the blade but in all the bridges we've made that its the promised glade the hill we climb if only we dare it because being american is more than a pride we inherit it's the past we step into and how we repair it we've seen a forest that would shatter a nation rather than share it would det our country if it meant delaying democracy and this effort very nearly succeeded but while democracy can be periodically delayed it can never be permanently defeated in this truth in this faith we trust for while we have our eyes on the future history has its eyes on us this is the era of just redemption we feared at itsinception we did not feel prepared to be the heirs of such a terrifying hour but within it we found the pouer to author a new chapter to offer hope in laughter to self so while once we asked how could we possibly prevail over catastrophe now we assert how could catastrophe possibly prevail over us we will not march back to what was but moved to what shall be a country that is bruised but whole benevolent but bold fierce and free we will not be turned around or interrupted by intimidation because we know our inaction and inertia will be the inheritance of the next generation our blunders become their dance but one thing is certain if we merge mercy with might in might with might then love becomes our legacy in change our children's birthright so let us leave behind a country better the one we left with every breath on my bronzed pounded chest we will raise this wounded world into a wondrous one we will rise from the gold limbed hills of the west we will rise from the wind swept northeast where our forefathers first realized revolution we will rise from the lake rimmed cities of the mid western states we will ise from the sun baked southebl we build reconcile and recover in every known nook of our nation in every corner called our country our people diverse and beautiful we'll emerge battered and beautiful when day comes we step out of the shade aflame and unafraid the new balloons as we free it for there is always light if only we're brave enough to see it if only we're brave enough to be it \n"
          ]
        }
      ],
      "source": [
        "print(poet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEbbDwc-ywov"
      },
      "outputs": [],
      "source": [
        "# Output the transcript to a text file if you wish.\n",
        "\n",
        "# with open(\"../transcripts/amanda_gorman_alt.txt\", \"w\") as file:\n",
        "#    file.write(poet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5KGZ_BnMhs9"
      },
      "source": [
        "## NOTE:\n",
        "\n",
        "2 minutes 44s might seem like a long time to transcribe something like this, but if you were to do this manually, it would probably take you twice as long, at least.\n",
        "\n",
        "Again, there are some obvious errors, but the results from the base model is pretty damn good if you ask me. As the models get more sophisticated, I'm pretty sure the quality of the output will get better."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "2.0_wav2vec2_long_speech.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}